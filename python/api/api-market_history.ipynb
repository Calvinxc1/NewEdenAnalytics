{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from datetime import timedelta, datetime, date\n",
    "import io\n",
    "import tarfile as tf\n",
    "import gzip as gz\n",
    "import pandas as pd\n",
    "\n",
    "from ApiHandler import ApiHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiMarketHistory(ApiHandler):\n",
    "    settings = {\n",
    "        **ApiHandler.settings,\n",
    "        'day_delay': 2,\n",
    "        'record_warning': 40000\n",
    "    }\n",
    "    url = {\n",
    "        **ApiHandler.url,\n",
    "        'data': 'https://storage.googleapis.com/evekit_md/{year:04}/{month:02}/{day:02}/market_{year:04}{month:02}{day:02}.tgz',\n",
    "    }\n",
    "    sql = {\n",
    "        **ApiHandler.sql,\n",
    "        'date_check': 'SELECT DISTINCT record_date FROM MarketHistory;'\n",
    "    }\n",
    "    script_vals = {\n",
    "        **ApiHandler.script_vals,\n",
    "        'table': 'MarketHistory'\n",
    "    }\n",
    "    name = 'EVEKit Market History API'\n",
    "    \n",
    "    @property\n",
    "    def data_date(self):\n",
    "        data_date = date.today() - timedelta(days=self.settings['day_delay'])\n",
    "        return data_date\n",
    "            \n",
    "    def get_raw_data(self, date=None):\n",
    "        if date is None: date = self.data_date\n",
    "        if self.verbose: self._verbose('get_raw_data', 'Getting raw data for date {date}...'.format(**{'date': date}))\n",
    "        \n",
    "        data_conn = rq.get(self.url['data'].format(**{\n",
    "            'year': date.year,\n",
    "            'month': date.month,\n",
    "            'day': date.day\n",
    "        }))\n",
    "        \n",
    "        if data_conn.status_code != 200:\n",
    "            raise Exception(\"\"\"\\\n",
    "                Connection returned a {status} code on pull for {date}.\n",
    "                Message body:\n",
    "                {body}\\\n",
    "            \"\"\".format(**{\n",
    "                'status':data_conn.status_code,\n",
    "                'date': date,\n",
    "                'body':data_conn.content.decode('utf-8')\n",
    "            }))\n",
    "            \n",
    "        if self.verbose: self._verbose('get_raw_data', 'Raw data acquired.')\n",
    "        return data_conn.content\n",
    "            \n",
    "    def build_data(self, raw_data):\n",
    "        if self.verbose: self._verbose('build_data', 'Building data frame...')\n",
    "        tar_file = tf.open(fileobj=io.BytesIO(raw_data))\n",
    "    \n",
    "        data_frame = pd.concat([\n",
    "            self._parse_data(tar_file.extractfile(type_file).read())\n",
    "            for type_file in tar_file\n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "        data_frame['record_date'] = pd.to_datetime(data_frame['record_date']/1000,unit='s').dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        if self.verbose: self._verbose('build_data', 'Data frame built. %s records.' % len(data_frame))\n",
    "            \n",
    "        if len(data_frame) < self.settings['record_warning']:\n",
    "            if self.verbose:\n",
    "                self._verbose(\n",
    "                    'build_data', \n",
    "                    'Data frame is {records}, which is under warning threshold of {record_warning} records. Sending e-mail.'.format(**{\n",
    "                        'records': len(data_frame),\n",
    "                        'record_warning': self.settings['record_warning']\n",
    "                    })\n",
    "                )\n",
    "            self._email(\n",
    "                'warning',\n",
    "                \"\"\"\\\n",
    "                Data extracted for date {date} has {records}, which is below the threshold of {record_warning} records.\n",
    "                This is a warning, not an error. Barring no other problems, the process will complete sucessfully.\\\n",
    "                \"\"\".format(**{\n",
    "                    'date': data_frame['record_date'].unique()[0],\n",
    "                    'records': len(data_frame),\n",
    "                    'record_warning': self.settings['record_warning']\n",
    "                })\n",
    "            )\n",
    "            \n",
    "        error_rows = data_frame['avg_price'] > data_frame['high_price']\n",
    "        if error_rows.sum() > 0:\n",
    "            if self.verbose:\n",
    "                self._verbose(\n",
    "                    'build_data', \n",
    "                    'Data frame has {records} with flipped avg_price and high_price. Correcting & sending e-mail.'.format(**{\n",
    "                        'records': error_rows.sum(),\n",
    "                        'record_warning': self.settings['record_warning']\n",
    "                    })\n",
    "                )\n",
    "            self._email(\n",
    "                'warning',\n",
    "                \"\"\"\\\n",
    "                Data extracted for date {date} has {records} with flipped avg_price and high_price. Values are corrected.\n",
    "                This is a warning, not an error. Barring no other problems, the process will complete sucessfully.\\\n",
    "                \"\"\".format(**{\n",
    "                    'date': data_frame['record_date'].unique()[0],\n",
    "                    'records': error_rows.sum(),\n",
    "                    'record_warning': self.settings['record_warning']\n",
    "                })\n",
    "            )\n",
    "            data_frame.loc[error_rows] = data_frame.loc[error_rows].rename(columns={'avg_price':'high_price', 'high_price':'avg_price'})\n",
    "            \n",
    "        return data_frame\n",
    "        \n",
    "    def _parse_data(self, type_file):\n",
    "        data_file = pd.read_csv(\n",
    "            io.BytesIO(gz.open(io.BytesIO(type_file)).read()),\n",
    "            header=None, index_col=None,\n",
    "            names=('type_id','region_id','order_count','low_price','avg_price','high_price','volume','record_date')\n",
    "        )\n",
    "        return data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ApiMarketHistory()\n",
    "#api.run_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "#for date_val in tqdm.tqdm_notebook(pd.date_range(date(2019,7,27), date(2019,8,23))):\n",
    "raw_data = api.get_raw_data(date(2019,9,18))\n",
    "data_frame = api.build_data(raw_data)\n",
    "api.insert_data(data_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
